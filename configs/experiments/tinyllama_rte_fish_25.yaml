# configs/experiments/tinyllama_rottentomatoes_fish_25.yaml
# -----------------------------------------------------------------------------
# Experiment: Fine-tune TinyLlama-1.1B on Rotten Tomatoes for sentiment analysis.
# Goal: Compare standard LoRA with FISH-Tuning pruned to 25% of baseline params.
#
# THIS CONFIG IS DESIGNED TO WORK WITH THE ORIGINAL, UNMODIFIED PYTHON SCRIPTS.
# It solves the previous ValueError by using a dataset (rotten_tomatoes) with
# a simple structure that the scripts can handle without modification.
# -----------------------------------------------------------------------------

model:
  name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  config_overrides:
    pad_token_id: 0 # TinyLlama tokenizer does not have a pad token by default

dataset:
  name: "rotten_tomatoes"
  load_args: ["rotten_tomatoes"]  # Load the Rotten Tomatoes dataset
  text_column: "text"             # The main text column in this dataset
  validation_split: "validation"  # It has a standard validation split
  label_column: "label"           # The label column is named 'label'

lora:
  baseline_rank: 8
  fish_rank: 32
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
  dropout: 0.05
  names_to_exclude:
    - "score"

training:
  output_dir: "./results/tinyllama-rotten-tomatoes"
  max_seq_length: 128 # Movie reviews are short, 128 is sufficient
  
  args_override:
    num_train_epochs: 3 # 3 epochs is enough for this dataset
    per_device_train_batch_size: 4
    gradient_accumulation_steps: 4
    evaluation_strategy: "epoch"
    save_strategy: "epoch"
    learning_rate: 0.00002
    weight_decay: 0.01
    lr_scheduler_type: "cosine"
    warmup_ratio: 0.1
    load_best_model_at_end: true
    metric_for_best_model: "accuracy"
    logging_steps: 50
    # The code modification for quantization is still highly recommended for speed,
    # but this config will run (slowly) without it.
    # If you made the quantization change, keep fp16 commented out.
    # If your code is completely original, uncomment fp16.
    fp16: true 

fish_tuning:
  methods_to_run: ["fish"]
  prune_to_ratio_of_baseline: 0.25
  num_samples: 128

base_training:
  args:
    report_to: "none"
    do_train: true
    do_eval: true
    per_device_eval_batch_size: 8
