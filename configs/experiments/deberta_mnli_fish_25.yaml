# configs/experiments/deberta_mnli_fish_25.yaml
# -----------------------------------------------------------------------------
# EXPERIMENT CONFIGURATION for DeBERTa-v3 on MNLI
# Format: Strictly follows the prajjwal_tinymodel_smoketest.yaml structure.
# -----------------------------------------------------------------------------

model:
  # This corresponds to model_args.model_name_or_path in your script
  name: "microsoft/deberta-v3-base"

dataset:
  # This corresponds to data_args.task_name
  name: "mnli"
  # These are the arguments for `datasets.load_dataset()`
  load_args: ["glue", "mnli"]
  # The script will likely need to know the keys for MNLI sentences
  # These are "premise" and "hypothesis". We add them here for the script to use.
  text_column: "premise"
  text_pair_column: "hypothesis"
  validation_split: "validation_matched" # MNLI has a specific validation split

lora:
  # Since your script likely uses 'fish_rank' for the main run and 'baseline_rank'
  # for a baseline run, we set them here. We are not running a baseline,
  # so 'baseline_rank' can be a placeholder.
  baseline_rank: 16 # Placeholder, not used in a FISH-only run
  fish_rank: 32     # The LoRA rank for our FISH-Tuning experiment. 16 is a good choice.
  lora_alpha: 64    # Standard is 2 * rank
  dropout: 0.05
  # Target modules specific to DeBERTa-v3 model
  target_modules:
    - "query_proj"
    - "key_proj"
    - "value_proj"
    - "dense"
  # Parameters to exclude from Fisher masking (always train densely)
  names_to_exclude:
    - "classifier"
    - "pooler"

training:
  # Max sequence length for MNLI. DeBERTa can handle longer sequences well.
  max_seq_length: 512
  output_dir: "./results/deberta-v3-base-mnli-fish-25"
  # All Hugging Face TrainingArguments go inside this 'args_override' block
  args_override:
    # -- Core Hyperparameters --
    num_train_epochs: 3
    per_device_train_batch_size: 16
    per_device_eval_batch_size: 32
    learning_rate: 0.00002
    weight_decay: 0.01

    # -- Directory and Logging --
    overwrite_output_dir: true
    logging_steps: 500
    report_to: "none"

    # -- Scheduler --
    warmup_ratio: 0.1
    lr_scheduler_type: "cosine"

    # -- Evaluation and Saving --
    evaluation_strategy: "steps"
    eval_steps: 1000
    save_strategy: "steps"
    save_steps: 1000
    save_total_limit: 2
    load_best_model_at_end: true
    metric_for_best_model: "accuracy"
    greater_is_better: true # Must be explicitly set

    # -- Other Settings --
    seed: 42
    fp16: true

fish_tuning:
  # Assuming your script can run a single method.
  # We are only running the main "fish" method here.
  methods_to_run: ["fish"]
  # Number of samples for Fisher calculation.
  num_samples: 2048
  # THIS IS THE CRITICAL MAPPING:
  # The smoke test uses 'prune_to_ratio_of_baseline'.
  # Here, we interpret it as the FINAL percentage of parameters to KEEP.
  # To keep 25%, we set this to 0.25. Your script needs to handle this logic.
  prune_to_ratio_of_baseline: 0.25
