# configs/experiments/minilm_rte_fish_25.yaml
# -----------------------------------------------------------------------------
# EXPERIMENT CONFIGURATION for MiniLMv2 on RTE
# A very fast, moderate-sized experiment that is compatible with the script's logic.
# Format: Strictly follows the prajjwal_tinymodel_smoketest.yaml structure.
# -----------------------------------------------------------------------------

model:
  name: "microsoft/MiniLM-L12-H384-uncased"

dataset:
  # Switched to RTE. It's a binary entailment task and is very small.
  name: "rte"
  load_args: ["glue", "rte"]
  # We remove text_column/text_pair_column. The script will auto-detect
  # "sentence1" and "sentence2", which are the correct columns for RTE.
  validation_split: "validation"

lora:
  baseline_rank: 8 # Placeholder
  fish_rank: 32
  lora_alpha: 64
  dropout: 0.05
  target_modules:
    - "query"
    - "key"
    - "value"
  names_to_exclude:
    - "classifier"

training:
  max_seq_length: 128
  output_dir: "./results/minilm-rte-fish-25"
  args_override:
    # -- Core Hyperparameters --
    # RTE is very small, so it can overfit. 3-5 epochs is typical.
    num_train_epochs: 7
    # We can use a large batch size because the dataset is tiny.
    per_device_train_batch_size: 32
    per_device_eval_batch_size: 64
    learning_rate: 0.00003
    weight_decay: 0.01

    # -- Directory and Logging --
    overwrite_output_dir: true
    logging_steps: 20
    report_to: "none"

    # -- Evaluation and Saving --
    # An epoch is only ~77 steps. Evaluate every 40 steps.
    evaluation_strategy: "steps"
    eval_steps: 40
    save_strategy: "steps"
    save_steps: 40
    save_total_limit: 2
    load_best_model_at_end: true
    metric_for_best_model: "accuracy"
    greater_is_better: true

    # -- Other Settings --
    seed: 42
    fp16: true

fish_tuning:
  methods_to_run: ["fish"]
  # The dataset is small, so we need fewer samples for the Fisher calculation.
  num_samples: 1024
  # Keeping the top 25% of LoRA parameters.
  prune_to_ratio_of_baseline: 0.25
